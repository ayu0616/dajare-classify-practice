{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Wordsを作ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"\"\"布団が吹っ飛んだ\n",
    "今日と明日は京都に行きます\n",
    "今日は雨が降っています\n",
    "授業は明日の午後からです\"\"\".splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数たち\n",
    "\n",
    "EOS = \"EOS\"\n",
    "KOMOJI_KANA = \"ァィゥェォャュョ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list: list[str] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger  = MeCab.Tagger(\"-Ochasen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in documents:\n",
    "    parsed: str = tagger.parse(sentence)\n",
    "    for line in parsed.splitlines()[:-1]:\n",
    "        word: str = line.split()[0]\n",
    "        word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id: dict[str, int] = dict()\n",
    "for word in word_list:\n",
    "    if word not in word_to_id:\n",
    "        word_to_id[word] = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bow(sentence: str):\n",
    "    bow = [0] * len(word_to_id)\n",
    "    parsed = tagger.parse(sentence)\n",
    "    for line in parsed.splitlines()[:-1]:\n",
    "        word = line.split()[0]\n",
    "        if word in word_to_id:\n",
    "            bow[word_to_id[word]] = 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = gen_bow(\"今日私がいただくのは、名店のロールキャベツです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 読み仮名列完全一致区間を探したい"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章の読み仮名を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 子音の音韻類似度を計算する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 読み仮名をローマ字に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import romkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mora(kana: str):\n",
    "    \"\"\"カタカナの文字列をモーラに分割する\"\"\"\n",
    "    mora_list:list[str] = []\n",
    "    for char in kana:\n",
    "        if char in KOMOJI_KANA:\n",
    "            mora_list[-1] += char\n",
    "        else:\n",
    "            mora_list.append(char)\n",
    "    return mora_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fu', 'to', 'n']\n",
      "['ga']\n",
      "['fu', 'xtsu', 'to', 'n']\n",
      "['da']\n",
      "['kyo', 'u']\n",
      "['to']\n",
      "['a', 'shi', 'ta']\n",
      "['ha']\n",
      "['kyo', 'u', 'to']\n",
      "['ni']\n",
      "['i', 'ki']\n",
      "['ma', 'su']\n",
      "['kyo', 'u']\n",
      "['ha']\n",
      "['a', 'me']\n",
      "['ga']\n",
      "['fu', 'xtsu']\n",
      "['te']\n",
      "['i']\n",
      "['ma', 'su']\n",
      "['ju', 'gyo', 'u']\n",
      "['ha']\n",
      "['a', 'shi', 'ta']\n",
      "['no']\n",
      "['go', 'go']\n",
      "['ka', 'ra']\n",
      "['de', 'su']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    parsed = tagger.parse(sentence)\n",
    "    for line in parsed.splitlines()[:-1]:\n",
    "        word = line.split()[1]\n",
    "        print(list(map(romkan.to_roma, split_mora(word))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
